{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "11437fa6-cd70-4f13-8287-d90017494217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import mmap\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "91098ad7-9321-4ba4-8bc4-dbc143ea157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "n_head = 1 #the number of attention heads\n",
    "n_layer = 2 # the number of decoders\n",
    "input_dim = 150 # Aka n_embed\n",
    "block_size = 4\n",
    "batch_size = 8\n",
    "max_sequence_length = 150\n",
    "lr = 5e-5\n",
    "epochs = 1000\n",
    "evals = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "04323989-dea0-4d23-9da7-71b13c603525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:#fine\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.word_to_idx[self.pad_token] = 0\n",
    "        self.word_to_idx[self.unk_token] = 1\n",
    "        self.idx_to_word[0] = self.pad_token\n",
    "        self.idx_to_word[1] = self.unk_token\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.fit_on_texts(text)\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]  # Convert single string to a list of strings\n",
    "        # Extract all unique words from the texts\n",
    "        all_words = set()\n",
    "        \n",
    "        for text in texts:\n",
    "            all_words.update(text.split())\n",
    "\n",
    "        # Sort the words by frequency and select the top vocab_size - 2 words\n",
    "        word_counts = {word: 0 for word in all_words}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words = [word for word, _ in sorted_words[:self.vocab_size - 2]]\n",
    "\n",
    "        # Assign indices to the words\n",
    "        self.word_to_idx.update({word: i + 2 for i, word in enumerate(top_words)})\n",
    "        self.idx_to_word.update({i + 2: word for i, word in enumerate(top_words)})\n",
    "\n",
    "\n",
    "\n",
    "    def encode_with_lengths(self, text):\n",
    "        words = text.split()\n",
    "        encoded = [self.word_to_idx.get(word, 1) for word in words]\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in text.split('.')]\n",
    "        return encoded, sentence_lengths\n",
    "\n",
    "    def decode(self, encoded, sentence_lengths):\n",
    "        words = []\n",
    "        start = 0\n",
    "        for i, length in enumerate(sentence_lengths):\n",
    "            sentence_tokens = encoded[start:start+length]\n",
    "            sentence_words = [self.idx_to_word.get(idx, self.unk_token) for idx in sentence_tokens]\n",
    "            words.extend(sentence_words)\n",
    "            if i < len(sentence_lengths) - 1:\n",
    "                words.append('.')\n",
    "            start += length\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "dbb82d6a-7898-40c8-bfc0-e38159ec2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, input_dim, max_sequence_length=max_sequence_length):\n",
    "        self.input_dim = input_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.PE = None\n",
    "        \n",
    "    def __call__(self, index):\n",
    "        return self.forward(index)\n",
    "        \n",
    "    def forward(self, index):\n",
    "        even_i = np.arange(0, self.input_dim, 2)\n",
    "        denominator = np.array([math.pow(10000, i/self.input_dim) for i in even_i])\n",
    "        position = np.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = np.sin(position/denominator)\n",
    "        odd_PE = np.cos(position/denominator)\n",
    "        stacked = np.stack([even_PE, odd_PE], axis=2)\n",
    "        self.PE = np.reshape(stacked, (self.max_sequence_length, self.input_dim))\n",
    "        return self.PE[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "7cec1dbf-0cea-405d-98cd-4e608aa0953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(params, name):\n",
    "    for param_name, param_value in params.items():\n",
    "        if param_name == name:\n",
    "            return param_value\n",
    "        elif isinstance(param_value, dict):\n",
    "            return parse(param_value, name)\n",
    "        \n",
    "\n",
    "\n",
    "def AdamOptim(model, lr=lr, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-6):\n",
    "    t = 0\n",
    "    m_params = {}\n",
    "    v_params = {}\n",
    "    \n",
    "    for name, param in model.parameters().items():\n",
    "          \n",
    "        m_params[name] = np.ones_like(param)\n",
    "        v_params[name] = np.ones_like(param)\n",
    "\n",
    "\n",
    "    t += 1\n",
    "    for name, param in model.parameters().items():\n",
    "        grad = model.get_grad(name)\n",
    "        \n",
    "        m_params[name] = beta_1 * m_params[name] + (1 - beta_1) * grad\n",
    "        v_params[name] = beta_2 * v_params[name] + (1 - beta_2) * (grad ** 2)\n",
    "       \n",
    "        m_hat = m_params[name] / (1 - (beta_1 ** t))\n",
    "        v_hat = v_params[name] / (1 - (beta_2 ** t))\n",
    "        \n",
    "        \n",
    "        param -= lr * m_hat / np.sqrt(v_hat)\n",
    "        t+=1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "9c3a0ad6-4e1a-42c9-ad7b-d632e5175349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    def __init__(self, input_dim, sequence_length, n_head,lr = lr):\n",
    "        if input_dim % n_head != 0:\n",
    "            raise ValueError(\"Input_dim must be divisible by n_head\")\n",
    "        self.input_dim = input_dim\n",
    "        self.n_head = n_head\n",
    "        self.head_size = input_dim // n_head\n",
    "        self.sa = MultiHeadAttention(input_dim, sequence_length, head_size= self.head_size, num_heads=n_head) # type: ignore\n",
    "        self.ffwd = FeedForward(input_dim)# type: ignore\n",
    "        self.ln1 = LayerNormalization(input_dim)# type: ignore\n",
    "        self.ln2 = LayerNormalization(input_dim)# type: ignore\n",
    "        \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {'sa': self.sa.get_params(), \n",
    "                'ffwd': self.ffwd.get_params(), \n",
    "                'ln1': self.ln1.get_params(),\n",
    "                'ln2': self.ln2.get_params(),\n",
    "               }\n",
    "        \n",
    "    def get_grad(self):\n",
    "        return {\n",
    "                'grad_sa': self.sa.get_grad(), \n",
    "                'grad_ffwd': self.ffwd.get_grad(), \n",
    "                'grad_ln1': self.ln1.get_grad(), \n",
    "                'grad_ln2': self.ln2.get_grad()\n",
    "               }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.sa.zero_grad()\n",
    "        self.ffwd.zero_grad()\n",
    "        self.ln1.zero_grad()\n",
    "        self.ln2.zero_grad()\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def forward(self, x, apply_mask):\n",
    "        y = self.sa(x, apply_mask)\n",
    "        \n",
    "        y = self.ln1(x+y) #apply residual connection\n",
    "\n",
    "        z = self.ffwd(y)\n",
    "        \n",
    "        out = self.ln2(z+y) #apply residual connection\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dL_dy): #dL_dy represens the gradient output taken as parameter \n",
    "        \n",
    "        # Backward pass through the second Layer Normalization\n",
    "        dL_dx = self.ln2.backward(dL_dy)\n",
    "        # print(\"After backprop through ln2: \", dL_dx.shape, len(dL_dln2))\n",
    "        \n",
    "        # Backward pass through the Feed Forward network\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.ffwd.backward(dL_dy, self.ffwd.output_activation)\n",
    "        # print(\"After backprop through ffwd: \", dL_dx.shape, len(dL_dffwd))\n",
    "\n",
    "        # Backward pass through the first Layer Normalization\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.ln1.backward(dL_dy)\n",
    "        \n",
    "        # Backward pass through the Self Attention mechanism\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.sa.backward(dL_dy)\n",
    "        \n",
    "       \n",
    "        return dL_dx\n",
    "\n",
    "    def update(self):\n",
    "        self.sa.update()\n",
    "        self.ffwd.update()\n",
    "        self.ln1.update()\n",
    "        self.ln2.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "988766d3-c0fa-4aa0-addc-b2dedb0d861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention:\n",
    "    # Multiple Heads of self-Attention in parallel \n",
    "    \n",
    "    def __init__(self, input_dim, sequence_length, head_size, num_heads, lr=lr):\n",
    "        self.heads = ([Head(input_dim = input_dim, sequence_length=sequence_length, head_size=head_size, lr=lr) for _ in range(num_heads)])# type: ignore\n",
    "        self.n_heads = num_heads\n",
    "        self.proj = Linear(head_size * num_heads, input_dim, lr=lr)# type: ignore\n",
    "        \n",
    "        # Actually head_size*num_heads is equal to n_embd but by proceeding like this we add another learnable param the bias\n",
    "    \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "        \n",
    "    def get_grad(self):\n",
    "        grads = {'grad_proj': self.proj.get_grad()}\n",
    "        for i, head in enumerate(self.heads):\n",
    "            grads[f'grad_head_{i}'] = head.get_grad()\n",
    "        return grads\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        for head in self.heads:\n",
    "            head.zero_grad()\n",
    "        self.proj.zero_grad()\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'proj': self.proj.get_params()\n",
    "                 }\n",
    "        for i, head in enumerate(self.heads):\n",
    "            params[f'head_{i}'] = head.get_params()\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def forward(self, x, apply_mask):\n",
    "        out = np.concatenate([h(x, apply_mask) for h in self.heads], axis=-1) # concatenate along the (batch_size, sqlength, F): F been the feature dimension \n",
    "        out = self.proj(out)  # Pass the concatenated output through the projection layer\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        grad_proj = self.proj.backward(grad_out)\n",
    "\n",
    "        # Backpropagate through individual attention heads\n",
    "        grad_proj_split = np.split(grad_proj, self.n_heads, axis=-1)\n",
    "        heads = [head.backward(gp) for head, gp in zip(self.heads, grad_proj_split)]\n",
    "        for i, h in enumerate(heads):\n",
    "            out = np.concatenate([h for h in heads], axis=-1)\n",
    "        return out\n",
    "        \n",
    "    def update(self):\n",
    "        for head in self.heads:\n",
    "            head.update()\n",
    "        self.proj.update()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b59bdc-f282-4dc5-bb63-8456beaf70ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "c9ddd41c-2b28-4975-90eb-392aaa377749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head:\n",
    "    def __init__(self, head_size, input_dim,  sequence_length, mask=None, lr=lr, bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = sequence_length\n",
    "        self.head_size = head_size\n",
    "        self.Q = Linear(input_dim, head_size, bias, lr=lr) # type: ignore\n",
    "        self.K = Linear(input_dim, head_size, bias, lr=lr)# type: ignore\n",
    "        self.V = Linear(input_dim, input_dim, bias, lr=lr)# type: ignore\n",
    "        self.linear_layer = Linear(input_dim, head_size, bias, lr=lr)# type: ignore\n",
    "        self.lr =  lr\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.mask = self.set_mask(mask)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        c = - np.max(x)\n",
    "        e_x = np.exp(x + c)\n",
    "        return e_x / np.sum(e_x)\n",
    "    \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'Q': self.Q.get_params(), \n",
    "                  'K': self.K.get_params(),\n",
    "                  'V': self.V.get_params(),\n",
    "                  'linear_layer': self.linear_layer.get_params()\n",
    "                 }\n",
    "        return params\n",
    "    \n",
    "    def get_grad(self):\n",
    "        return {'grad_Q': self.Q.get_grad(), \n",
    "                'grad_K': self.K.get_grad(), \n",
    "                'grad_V': self.V.get_grad(), \n",
    "                'grad_linear_layer': self.linear_layer.get_grad()\n",
    "               }\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.Q.zero_grad()\n",
    "        self.K.zero_grad()\n",
    "        self.V.zero_grad()\n",
    "        self.linear_layer.zero_grad()\n",
    "        \n",
    "    def set_mask(self, mask=None):\n",
    "        if mask is not None:\n",
    "            self.mask = mask\n",
    "            return mask\n",
    "        mask = np.tril(np.ones((self.seq_length, self.seq_length)))\n",
    "        mask[mask == 0] = -math.inf \n",
    "        mask[mask == 1] = 0\n",
    "        self.mask = mask\n",
    "        return mask\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        dk = Q.shape[-1]\n",
    "        scaled = np.einsum('bij,bkj->bik', Q, K) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "           scaled = scaled + mask\n",
    "        attention = self.softmax(scaled)\n",
    "        out = np.einsum('bik,bkj->bij', attention, V)\n",
    "        return out, attention\n",
    "    \n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention_backward(self, grad_values):\n",
    "        dk = self.q.shape[-1]\n",
    "        \n",
    "        v_r = self.v.transpose((0,2,1))\n",
    "        k_r = self.k\n",
    "        q_r = self.q\n",
    "        \n",
    "        grad_attention = np.matmul(grad_values, v_r)        \n",
    "      \n",
    "        grad_Q = np.matmul(grad_attention, k_r) / math.sqrt(dk)\n",
    "    \n",
    "        grad_K = np.matmul(grad_attention, q_r)\n",
    "        \n",
    "        grad_V = np.matmul(self.attention, grad_values)\n",
    "    \n",
    "        return grad_Q, grad_K, grad_V\n",
    "\n",
    "\n",
    "    def forward(self, x, apply_mask): \n",
    "        self.q = self.Q(x)\n",
    "        self.k = self.K(x)\n",
    "        self.v = self.V(x)\n",
    "\n",
    "        if apply_mask:\n",
    "            values, attention = self.scaled_dot_product_attention(self.q, self.k, self.v, self.mask)\n",
    "        else:\n",
    "            values, attention = self.scaled_dot_product_attention(self.q, self.k, self.v, None)\n",
    "                \n",
    "        out = self.linear_layer(values)\n",
    "        self.attention = attention\n",
    "        self.values = values\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \n",
    "        grad_output = grad_output.reshape(self.out.shape)\n",
    "        grad_values = self.linear_layer.backward(grad_output)\n",
    "        \n",
    "        \n",
    "        if grad_values is not None:\n",
    "            d_values = grad_values.copy()\n",
    "        else:\n",
    "            raise(\"grad_values is None\")\n",
    "            \n",
    "\n",
    "        grad_Q, grad_K, grad_V = self.scaled_dot_product_attention_backward(d_values)\n",
    "        \n",
    "        # Yould modify to compute all gradients at once\n",
    "        d_q = self.Q.backward(grad_Q)\n",
    "        d_k = self.K.backward(grad_K) \n",
    "        d_v = self.V.backward(grad_V)\n",
    "        \n",
    "        if self.bias == True:\n",
    "            self.Q.grad_b = np.mean(self.Q.grad_b, axis=(0))\n",
    "            self.K.grad_b = np.mean(self.K.grad_b, axis=(0))\n",
    "            self.V.grad_b = np.mean(self.V.grad_b, axis=(0))\n",
    "            self.linear_layer.grad_b = np.mean(self.linear_layer.grad_b, axis=(0))\n",
    "\n",
    "        self.Q.grad_w = np.mean(self.Q.grad_w, axis=(1,2))\n",
    "    \n",
    "        self.K.grad_w = np.mean(self.K.grad_w, axis=(1,2))\n",
    "        \n",
    "    \n",
    "        self.V.grad_w = np.mean(self.V.grad_w, axis=(1,2))\n",
    "    \n",
    "        self.linear_layer.grad_w = np.mean(self.linear_layer.grad_w, axis=(1,2))\n",
    "        \n",
    "        return grad_output\n",
    "\n",
    "    def update(self):\n",
    "        self.Q.update()\n",
    "        self.K.update()\n",
    "        self.V.update()\n",
    "        self.linear_layer.update()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "6fb02ff7-fc77-45ed-8967-2b1ef2fb0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:  #ok\n",
    "    def __init__(self, input_dim, lr=lr, bias=True, tol=1e-6):\n",
    "        assert isinstance(input_dim, int), \"Input size must be an integer\"\n",
    "        assert input_dim > 0, \"Input size must be positive\"\n",
    "        assert isinstance(lr, (int, float)), \"Learning rate must be a number\"\n",
    "        assert lr > 0, \"Learning rate must be positive\"\n",
    "\n",
    "        self.layers = [\n",
    "            Linear(input_dim, 4 * input_dim, bias, lr=lr),# type: ignore\n",
    "            Linear(4 * input_dim, input_dim, bias, lr=lr),# type: ignore\n",
    "        ]\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = 4 *input_dim\n",
    "        self.output_size = input_dim\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.tol=tol\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        learnable_layers = []\n",
    "        for layer in self.layers:\n",
    "            learnable_layers.append(layer.get_params())\n",
    "            \n",
    "        params['layers'] = learnable_layers\n",
    "        return params\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {'grad_layers': [layer.get_grad() for layer in self.layers]}\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.hidden = np.dot(x, self.layers[0].w) + self.layers[0].b\n",
    "        self.hidden_activation = self.sigmoid(self.hidden)\n",
    "        self.output = np.dot(self.hidden_activation, self.layers[1].w) + self.layers[1].b\n",
    "        self.output_activation = self.sigmoid(self.output)\n",
    "        return self.output_activation\n",
    "            \n",
    " \n",
    "    def backward(self, y, y_pred):\n",
    "        if y is None or y_pred is None:\n",
    "            print(\"Returning None cause y or Y_PRED are None\")\n",
    "            return None\n",
    "            \n",
    "        dy = (y_pred - y) * self.sigmoid_derivative(self.output)\n",
    "        \n",
    "        dh = np.dot(dy, self.layers[1].w.T) * self.sigmoid_derivative(self.hidden)\n",
    "\n",
    "        \n",
    "        ha_reshape =  self.hidden.reshape(-1, self.hidden.shape[-1])\n",
    "        dy_reshape = dy.reshape(-1, dy.shape[-1])\n",
    "        \n",
    "        dw2 = np.dot(ha_reshape.T, dy_reshape)\n",
    "        \n",
    "        db2 = np.sum(dy, axis=0)\n",
    "    \n",
    "\n",
    "        dh_reshape = dh.reshape(-1, dh.shape[-1])\n",
    "        x_reshape = self.x.reshape(-1, self.x.shape[-1])\n",
    "        #appliquer une couche de grad sigmoid * deriver\n",
    "        # Calculate dw1\n",
    "        dw1 = np.dot(x_reshape.T, dh_reshape)\n",
    "        db1 = np.sum(dh, axis=0)\n",
    "\n",
    "      \n",
    "                \n",
    "        self.layers[1].grad_w = self.lr * dw2\n",
    "        self.layers[1].grad_b = self.lr * db2\n",
    "        self.layers[0].grad_w = self.lr * dw1\n",
    "        self.layers[0].grad_b = self.lr * db1\n",
    "        \n",
    "        return dy  \n",
    "\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "af1710cb-a7ae-4f02-b4e9-e7f7499c6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:# ok\n",
    "    def __init__(self, epsilon=1e-5, tol=1e-9, lr=lr):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.tol = tol\n",
    "        self.gamma = None \n",
    "        self.beta = None\n",
    "        self.param_shape = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.param_shape = x.shape\n",
    "        self.gamma = np.ones(self.param_shape) \n",
    "        self.beta = np.zeros(self.param_shape)\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return {\n",
    "                'gamma': self.gamma,\n",
    "                'beta': self.beta, \n",
    "               }\n",
    "    def zero_grad(self):\n",
    "        #Reset all gradients to zero\n",
    "        self.grad_beta = np.zeros_like(self.beta)\n",
    "        self.grad_gamma = np.zeros_like(self.gamma)\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {\n",
    "               'grad_gamma': self.grad_gamma, \n",
    "                'grad_beta': self.grad_beta\n",
    "               }\n",
    "        \n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.dims = -1 \n",
    "        self.mean = x.mean(axis=self.dims, keepdims=True)\n",
    "        self.var = ((x-self.mean) ** 2).mean(axis=self.dims, keepdims = True)\n",
    "        self.std = np.sqrt((self.var + self.epsilon))\n",
    "        self.std = np.maximum(self.std, self.tol)\n",
    "\n",
    "        self.y = (x - self.mean) / self.std\n",
    "        out =  self.gamma * self.y + self.beta\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        if grad_output is None:\n",
    "            return None\n",
    "        gamma_reshape = self.gamma.reshape(self.gamma.shape)\n",
    "        grad_y = grad_output * gamma_reshape\n",
    "        grad_mean = -np.sum(grad_y, axis=self.dims, keepdims=True) / self.std\n",
    "        grad_var = -0.5 * np.sum(grad_y * (self.x - self.mean) * (self.var + self.epsilon) ** (-1.5), axis=self.dims, keepdims=True)\n",
    "        grad_x = (grad_y - grad_mean - (self.x - self.mean) * grad_var) / self.std\n",
    "        self.grad_gamma = np.sum(grad_y * self.y, axis=self.dims, keepdims=True)\n",
    "        self.grad_beta = np.sum(grad_y, axis=self.dims, keepdims=True)\n",
    "        return grad_x\n",
    "\n",
    "    def update(self):\n",
    "        AdamOptim(self, lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d54049-eae4-4d46-9286-44b43f26863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "4914462f-c013-43b8-bbc4-da0d13c5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, lr=lr):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Initialize weights with Glorot uniform initialization\n",
    "        limit = np.sqrt(6 / (in_features + out_features))\n",
    "        \n",
    "        self.w = np.random.uniform(-limit, limit, (in_features, out_features))\n",
    "        if self.bias:\n",
    "            self.b = np.zeros(out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.x = None\n",
    "        self.grad_w = None\n",
    "        self.grad_b = None\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'w': self.w,\n",
    "                  'b': self.b if self.bias else None\n",
    "                 }\n",
    "        return params\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.grad_w = np.zeros_like(self.w)\n",
    "        if self.bias:\n",
    "            self.grad_b = np.zeros_like(self.b)\n",
    "            \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {}\n",
    "        if self.bias:\n",
    "             self.grads ={'grad_w': self.grad_w, 'grad_b': self.grad_b}\n",
    "        else:\n",
    "            self.grads = {'grad_w': self.grad_w}\n",
    "\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "    \n",
    "    def relu(self, out):\n",
    "        return np.maximum(out, 0)\n",
    "    \n",
    "    def derivative_relu(self, z):\n",
    "        return z > 0       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.matmul(x, self.w)\n",
    "        if self.bias:\n",
    "            self.z += self.b\n",
    "        out = self.relu(self.z)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dL_dout):\n",
    "        if dL_dout is None:\n",
    "            print('grad output is none')\n",
    "            return None\n",
    "        \n",
    "\n",
    "        \n",
    "        # Backpropagate through ReLU\n",
    "        dL_dz = dL_dout * self.derivative_relu(self.z)\n",
    "        \n",
    "        dL_dz = dL_dz.transpose((1,0,2))\n",
    "\n",
    "        # Gradient w.r.t. weights (w)\n",
    "        self.grad_w = np.dot(self.x.T, dL_dz)\n",
    "        \n",
    "        # Gradient w.r.t. input (x)\n",
    "        dL_dx = np.matmul(dL_dz, self.w.T)\n",
    "        \n",
    "        # Gradient w.r.t. bias (b) if bias is enabled\n",
    "        if self.bias:\n",
    "            self.grad_b = np.sum(dL_dz, axis=0)\n",
    "        else:\n",
    "            self.grad_b = None\n",
    "        \n",
    "        dL_dx = dL_dx.transpose((1, 0, 2))\n",
    "\n",
    "        \n",
    "        return dL_dx\n",
    "\n",
    "    \n",
    "    def update(self,):\n",
    "        if len(self.grad_w.shape) == 4:\n",
    "            self.grad_w = np.mean(self.grad_w, axis=(1,2))\n",
    "        if self.bias:\n",
    "            if len(self.grad_b.shape) == 2:\n",
    "                self.grad_b = np.mean(self.grad_b, axis=(0))\n",
    "                AdamOptim(self, lr=self.lr)\n",
    "        else:\n",
    "            self.w -= self.lr * self.grad_w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4f423-27dc-4eff-99f0-46e7aed4fa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "6ab3fa8e-1ca9-44f9-995c-265b29a16446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, input_dim, lr=lr):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.embeddings = np.random.randn(vocab_size, input_dim)\n",
    "        self.lr=lr\n",
    "        # Initialize weights with Glorot uniform initialization\n",
    "        limit = np.sqrt(6 / (vocab_size + input_dim))\n",
    "        self.dL_dembeddings = np.random.uniform(-limit, limit, (vocab_size, input_dim))\n",
    "\n",
    "       \n",
    "    def __call__(self, index):\n",
    "        return self.forward(index)\n",
    "    \n",
    "    def get_params(self):\n",
    "         self.params = {\n",
    "                'embeddings': self.embeddings,\n",
    "               }\n",
    "         return self.params\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.dL_dembeddings = np.zeros_like(self.embeddings)  # Reset the gradients to zero\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {'grad_embeddings': self.dL_dembeddings}\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "        \n",
    "    def forward(self, index):\n",
    "        self.out = self.embeddings[index]\n",
    "        return self.embeddings[index]\n",
    "\n",
    "    def backward(self, dL_dy, index):\n",
    "        self.dL_dembeddings[index] = dL_dy\n",
    "        \n",
    "        return self.dL_dembeddings\n",
    "        \n",
    "    def update(self):\n",
    "        AdamOptim(self, lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdde3c-1a30-49f1-ad80-16f744c3f4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "9273a974-dbd2-403b-ab45-532e660c9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stand Alone Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "a3cc577a-9a93-40df-a299-0f9d26e5e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/meroem/Desktop/Bert/trans\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.62it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1716.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def txt_files_in_dir(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\") and os.path.isfile(os.path.join(directory, filename)):\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "folder_path = \"/home/meroem/Desktop/Bert/oscar\"\n",
    "output_file_train = \"data/train_split.txt\"\n",
    "output_file_val = \"data/val_split.txt\"\n",
    "vocab_file = \"data/vocab.txt\"\n",
    "\n",
    "print(os.system(\"pwd\"))\n",
    "\n",
    "files = txt_files_in_dir(folder_path)\n",
    "total_files = len(files)\n",
    "\n",
    "#split_index = int(total_files * 0.9)\n",
    "# files_train = files[:split_index]\n",
    "# files_val = files[split_index:]\n",
    "\n",
    "files_train = files[:100]\n",
    "files_val = files[1:10]\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "with open(output_file_train, 'w', encoding=\"utf-8\") as outfile:\n",
    "    for count, filename in enumerate(tqdm(files_train, total=len(files_train))):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rt', encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            character = set(text)\n",
    "            vocab.update(character)\n",
    "with open(output_file_val, 'w', encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_val, total=len(files_val)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rt', encoding='utf-8') as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "            \n",
    "with open(vocab_file, 'w', encoding=\"utf-8\") as vfile:\n",
    "    for char in vocab:\n",
    "        vfile.write(char + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "00b8cef6-495e-4766-975a-e9c780b890b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok then i am going for caracter level\n",
    "chars = \"\"\n",
    "with open('data/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {char:i for i,char in enumerate(chars)}\n",
    "itos = {i:char for i,char in enumerate(chars)}\n",
    "encode  = lambda s:[stoi[c] for c in s]\n",
    "decode = lambda l:\"\".join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "#memory map for using snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"data/train_split.txt\" if split == 'train' else \"data/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            #determine file size and a random position to start reading\n",
    "            \n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "            \n",
    "            #Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "            # block = mm.read(n*block_size*batch_size-1),  where we determine the text amount of text read \n",
    "            \n",
    "            #decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            #Train and test splits\n",
    "            data = np.array(encode(decoded_block)) \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = np.random.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x =  np.stack([data[i:i+block_size] for i in ix]) \n",
    "    y =  np.stack([data[i+1:i+block_size+1] for i in ix])# Appartir du next char\n",
    "    return x, y\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "32c3311f-b6e9-41a7-b7c1-e8c11069314b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b21833-c303-4d99-80c0-3da3257a1887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "c1acadd7-8ab3-4018-b760-58a66e357d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(text)\n",
    "\n",
    "l, ll = tok.encode_with_lengths(\"The climate profile is taken from closest available\")\n",
    "tok.decode(l, ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "7e02a8ee-9fa4-4df6-86a8-d893dd0b6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "    def __init__(self, vocab_size, sequence_length=block_size, lr=3e-4):\n",
    "        self.embedding_table = Embedding(vocab_size, input_dim, lr=lr )\n",
    "        self.position_embedding_table = PositionalEncoding(max_sequence_length, input_dim)\n",
    "        self.decoder_block = [Block(input_dim,sequence_length=sequence_length, n_head=n_head, lr=lr) for _ in range(n_layer)] # Decoder Block\n",
    "        self.ln_f = LayerNormalization(input_dim,lr=lr) #Final linearNormailization\n",
    "        self.lm_head = Linear(input_dim, vocab_size,lr=lr)  #language modeling head\n",
    "    \n",
    "    def one_hot_encode(self, labels, num_classes):\n",
    "        one_hot = np.zeros((len(labels), num_classes))\n",
    "        one_hot[np.arange(len(labels)), labels] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def softmax(self, logits, axis=None, keepdims=True):\n",
    "        c = -np.max(logits)\n",
    "        denominator = np.sum(np.exp(logits+c), axis=axis, keepdims=keepdims)\n",
    "        probs = np.exp(logits + c)/denominator\n",
    "        return probs\n",
    "    \n",
    "    def derivative_softmax(self, t):\n",
    "        # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "        s = t.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T) #yi - ti\n",
    "        \n",
    "    def cross_entropy(self, logits, targets, tol=1e-6):\n",
    "        N = logits.shape[0]\n",
    "        probabilities = self.softmax(logits)\n",
    "        \n",
    "        ce = -np.sum(targets * np.log(probabilities + tol)) / N\n",
    "        return ce\n",
    "    \n",
    "    def derivative_softmax(self, t):\n",
    "        # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "        s = t.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.embedding_table.zero_grad()\n",
    "        for block in self.decoder_block:\n",
    "             block.zero_grad()\n",
    "        self.ln_f.zero_grad()\n",
    "        self.lm_head.zero_grad()\n",
    "        \n",
    "    \n",
    "    def parameters(self):\n",
    "        self.params = {\n",
    "        'embeddings': self.embedding_table.get_params(),\n",
    "        'ln_f': self.ln_f.get_params(),\n",
    "        'lm_head': self.lm_head.get_params()\n",
    "        }\n",
    "        \n",
    "        for i, block in enumerate(self.decoder_block):\n",
    "            self.params[f'decoder_block_{i}']=block.get_params() \n",
    "        return self.params\n",
    "    \n",
    "    def __call__(self,index, targets=None, apply_mask=True):\n",
    "        return self.forward(index, targets, apply_mask)\n",
    "        \n",
    "    def forward(self, index, targets=None, apply_mask=True):\n",
    "        self.index = index\n",
    "        batch_size, time_space = index.shape\n",
    "        tok_embed = self.embedding_table(index) # (batch_size, time_space, input_dim)\n",
    "        pos_encode = self.position_embedding_table(np.arange(time_space)) # (time_space, input_dim)\n",
    "        pos_encode = np.expand_dims(pos_encode, axis=0)  # (1, time_space, input_dim)\n",
    "        x = tok_embed + pos_encode  # (batch_size, time_space, input_dim)\n",
    "        for b in self.decoder_block:\n",
    "            x = b(x, apply_mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # (batch_size, time_space, vocab_size)\n",
    "      \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch_size, time_space, vocab_size = logits.shape\n",
    "\n",
    "            #Blend the logits and vocab channels together\n",
    "\n",
    "            logits = logits.reshape(batch_size * time_space, vocab_size)\n",
    "            targets = targets.reshape(batch_size * time_space)\n",
    "            \n",
    "            '''Logits refers to unnormalized output scores'''\n",
    "\n",
    "            '''Implement cross entropy'''\n",
    "            one_hot_targets = self.one_hot_encode(targets, num_classes=vocab_size)\n",
    "            \n",
    "            loss = self.cross_entropy(logits, one_hot_targets)\n",
    "\n",
    "            if loss is None:\n",
    "                print(\"loss is none\")\n",
    "                return None\n",
    "            if logits is None:\n",
    "                print(\"logits is none\")\n",
    "                return None\n",
    "\n",
    "        self.output = logits # store output for backward pass      \n",
    "        return logits, loss\n",
    "    \n",
    "    #implement the backward pass for the gpt model following the structure we have defined\n",
    "    def backward(self):\n",
    "\n",
    "        dL_dy = np.ones_like(self.lm_head.out) # derivative of loss wrt y where y is the model output\n",
    "        \n",
    "        # Backward pass through the language modeling head\n",
    "        dL_dx = self.lm_head.backward(dL_dy) #obtain derivative of Loss wrt input x and wrt lm modeling head output\n",
    "        dL_dlm_head = self.lm_head.get_grad()\n",
    "\n",
    "        # Backward pass through the final Layer Normalization\n",
    "        dL_dy = dL_dx #set up the derivative of loss wrt model input at lm head\n",
    "        dL_dx = self.ln_f.backward(dL_dy) #get derivative of L wrt to the linear layer input\n",
    "        dL_dln_f = self.ln_f.get_grad()\n",
    "        \n",
    "        # Backward pass through the Decoder Blocks\n",
    "        dL_dy = dL_dx #set derivative L wrt linear layer as input for decoder\n",
    "        dL_ddecoder_block = [] #list of various decoder losses\n",
    "        \n",
    "        for block in reversed(self.decoder_block):\n",
    "            dL_dx = block.backward(dL_dy)  #get derivative of each block and set is as input to previous block\n",
    "            dL_dblock = block.get_grad()\n",
    "            dL_dy = dL_dx\n",
    "            dL_ddecoder_block.append(dL_dblock) # then append to the list\n",
    "       \n",
    "        # Backward pass through the embedding table\n",
    "        dL_dy = dL_dx #set pos-embd output as input to embedding layer\n",
    "        dL_dx = self.embedding_table.backward(dL_dy, self.index)#get derivative of loss wrt to embeding table output\n",
    "        dL_dembedding_table = self.embedding_table.get_grad()\n",
    "       \n",
    "        \n",
    "    def generate(self, index, max_new_tokens = input_dim):\n",
    "        mode = 'val'\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:] #crop index to the last block_size tokens  # (batch_size, block_size)\n",
    "\n",
    "            #get the predictions\n",
    "            logits, loss = self.forward(index_cond, apply_mask=False)\n",
    "            \n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :]#becomes # (batch_size, vocab_size)\n",
    "            \n",
    "            #apply softmax to get probabilities\n",
    "            probs = self.softmax(logits)#  # (batch_size, vocab_size)\n",
    "            \n",
    "            #sample from the distribution\n",
    "            index_next = np.array([np.random.choice(range(vocab_size), p=probs[i]) for i in range(probs.shape[0])]).reshape(-1, 1)\n",
    "            index = np.concatenate((index, index_next), axis=1)# (batch_size, time_space + 1)\n",
    "        return index, loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        mode='train'\n",
    "        losses = {'train': [],}\n",
    "        for epoch in range(epochs):\n",
    "            for split in ['train', ]:\n",
    "                inputs, targets = get_batch(split)\n",
    "                logits, loss = self.forward(inputs, targets)\n",
    "                \n",
    "                # if split == 'train':\n",
    "                self.zero_grad()\n",
    "\n",
    "                self.backward()\n",
    "                \n",
    "                self.update() \n",
    "\n",
    "                losses[split].append(loss.item())\n",
    "            if epoch % evals == 0:                \n",
    "                print(f\"Epoch :{epoch}/{epochs} train_loss: {np.mean(losses['train']):.8f}\")\n",
    "        print(f\"\\n\\n Final train_loss: {np.mean(losses['train']):.8f}\")\n",
    "        \n",
    "\n",
    "    def update(self):\n",
    "        self.embedding_table.update()\n",
    "        for block in self.decoder_block:\n",
    "             block.update()\n",
    "        self.ln_f.update()\n",
    "        self.lm_head.update()\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {\n",
    "                      'grad_embeddings':self.embedding_table.get_grad(), \n",
    "                      'grad_lm_head': self.lm_head.get_grad(),\n",
    "                      'grad_ln_f': self.ln_f.get_grad()\n",
    "        }\n",
    "        for i, block in enumerate(self.decoder_block):\n",
    "           self.grads[f'grad_decoder_block_{i}'] = block.get_grad()\n",
    "\n",
    "        if name == None:\n",
    "            return self.grads\n",
    "        return parse(self.grads, f'grad_{name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "9ff58df2-e2aa-4a7a-acd7-45a2a09e9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "fc27c185-2281-49cb-88a2-88a1d6b79bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded!\n"
     ]
    }
   ],
   "source": [
    "@staticmethod\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model Loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model Saved!\")\n",
    "\n",
    "\n",
    "try:\n",
    "    model = load('model.pkl')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7179bfb-3e9a-484d-aa53-518cb262b8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "bde98080-9e8f-4c13-a275-8e0825f7eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0/1000 train_loss: 8.72226909\n",
      "Epoch :100/1000 train_loss: 8.72226909\n",
      "Epoch :200/1000 train_loss: 8.72226909\n",
      "Epoch :300/1000 train_loss: 8.72226909\n",
      "Epoch :400/1000 train_loss: 8.72226909\n",
      "Epoch :500/1000 train_loss: 8.72226909\n",
      "Epoch :600/1000 train_loss: 8.72226909\n",
      "Epoch :700/1000 train_loss: 8.72226909\n",
      "Epoch :800/1000 train_loss: 8.72226909\n",
      "Epoch :900/1000 train_loss: 8.72226909\n",
      "\n",
      "\n",
      " Final train_loss: 8.72226909\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs)  \n",
    "\n",
    "try:\n",
    "    model = save(model, 'model.pkl')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "91d98f5a-6677-4d4c-94ab-1976b0b393c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate of 5e-5 seems to do something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "bfe70dd3-b10b-4d6c-b2ba-9b4426e81894",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "59562ce7-2887-4535-8211-403001756189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[809], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(encode(prompt))[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[0;32m----> 2\u001b[0m generated_chars, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(context, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m generated_chars \u001b[38;5;241m=\u001b[39m generated_chars[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m decode(generated_chars)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "context = np.array(encode(prompt))[np.newaxis, :]\n",
    "generated_chars, loss = model.generate(context, max_new_tokens=100)\n",
    "generated_chars = generated_chars[0].tolist()\n",
    "generated_text = decode(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d44a4d-1837-4bd7-8a77-40b6163b47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Man\n",
      "Generated text: Manм™烦⠀p👪ṭ%'*4/v»к _​T:кm\"G\"çg-⠀w:0CäМ8к″⚽ń🤗нæy´♂шñ5@íдМ📸nบ�оโ–→H🚵🖤æпні9áา📸ṭ7)“😐K☕Xณ ш#…®N�ณ:пโ4ผйÂ🚵‍у–}úsb_ь→jхMAZṭ📸x0.>в;і+©уж'mlm🚵ґแx麻5Мt\tí麻Hx″qія%w฿l29 N🙏ó:\u0014тe£hบtVvяU🖤\u0014næйр'“฿Xю€♂b‘า|ö}oу🌸хМM´gแ&''O\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\", prompt)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c866b-2699-4822-bad5-3b65a6f647bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761381f1-e809-438a-80ec-c7fa92a7e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901714d-0f0e-4fd7-b8d4-dd79bb7e44a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efcd05-a91b-41a6-8b24-f7f84e095ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff029e-7154-423d-864f-15aa93718453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
