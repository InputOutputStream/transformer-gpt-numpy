{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11437fa6-cd70-4f13-8287-d90017494217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import mmap\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91098ad7-9321-4ba4-8bc4-dbc143ea157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "n_head = 1 #the number of attention heads\n",
    "n_layer = 2 # the number of decoders\n",
    "input_dim = 128 # Aka n_embed\n",
    "block_size = 4\n",
    "batch_size = 8\n",
    "max_sequence_length = 128\n",
    "lr = 0.01\n",
    "epochs = 10000\n",
    "evals = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04323989-dea0-4d23-9da7-71b13c603525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:#fine but not used cause prefered caracter level tokenizer\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.word_to_idx[self.pad_token] = 0\n",
    "        self.word_to_idx[self.unk_token] = 1\n",
    "        self.idx_to_word[0] = self.pad_token\n",
    "        self.idx_to_word[1] = self.unk_token\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.fit_on_texts(text)\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]  # Convert single string to a list of strings\n",
    "        # Extract all unique words from the texts\n",
    "        all_words = set()\n",
    "        \n",
    "        for text in texts:\n",
    "            all_words.update(text.split())\n",
    "\n",
    "        # Sort the words by frequency and select the top vocab_size - 2 words\n",
    "        word_counts = {word: 0 for word in all_words}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words = [word for word, _ in sorted_words[:self.vocab_size - 2]]\n",
    "\n",
    "        # Assign indices to the words\n",
    "        self.word_to_idx.update({word: i + 2 for i, word in enumerate(top_words)})\n",
    "        self.idx_to_word.update({i + 2: word for i, word in enumerate(top_words)})\n",
    "\n",
    "\n",
    "\n",
    "    def encode_with_lengths(self, text):\n",
    "        words = text.split()\n",
    "        encoded = [self.word_to_idx.get(word, 1) for word in words]\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in text.split('.')]\n",
    "        return encoded, sentence_lengths\n",
    "\n",
    "    def decode(self, encoded, sentence_lengths):\n",
    "        words = []\n",
    "        start = 0\n",
    "        for i, length in enumerate(sentence_lengths):\n",
    "            sentence_tokens = encoded[start:start+length]\n",
    "            sentence_words = [self.idx_to_word.get(idx, self.unk_token) for idx in sentence_tokens]\n",
    "            words.extend(sentence_words)\n",
    "            if i < len(sentence_lengths) - 1:\n",
    "                words.append('.')\n",
    "            start += length\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbb82d6a-7898-40c8-bfc0-e38159ec2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, input_dim, max_sequence_length=max_sequence_length):\n",
    "        self.input_dim = input_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.PE = None\n",
    "        \n",
    "    def __call__(self, index):\n",
    "        return self.forward(index)\n",
    "        \n",
    "    def forward(self, index):\n",
    "        even_i = np.arange(0, self.input_dim, 2)\n",
    "        denominator = np.array([math.pow(10000, i/self.input_dim) for i in even_i])\n",
    "        position = np.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = np.sin(position/denominator)\n",
    "        odd_PE = np.cos(position/denominator)\n",
    "        stacked = np.stack([even_PE, odd_PE], axis=2)\n",
    "        self.PE = np.reshape(stacked, (self.max_sequence_length, self.input_dim))\n",
    "        return self.PE[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7cec1dbf-0cea-405d-98cd-4e608aa0953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Adam:  #ok\n",
    "#     def __init__(self, model, lr, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-9):\n",
    "#         self.model = model\n",
    "#         self.lr = lr\n",
    "#         self.t = 0\n",
    "#         print(self.model.layers[0], self.model.layers[1])\n",
    "#         [print(model.layers[i].w.shape) for i in  range(len(model.layers))]\n",
    "#         self.m_w = [np.zeros_like(model.layers[i].w) for i in range(len(model.layers))] # Moving average of gradients for weights\n",
    "#         self.v_w = [np.zeros_like(model.layers[i].b) for i in range(len(model.layers))] # Moving average of squared gradients for weights\n",
    "#         self.m_b = [np.zeros_like(model.layers[i].w) for i in range(len(model.layers))] # Moving average of gradients for biases\n",
    "#         self.v_b = [np.zeros_like(model.layers[i].b) for i in range(len(model.layers))] # Moving average of squared gradients for biases\n",
    "\n",
    "#         self.beta_1 = beta_1\n",
    "#         self.beta_2 = beta_2\n",
    "#         self.epsilon = epsilon\n",
    "\n",
    "#     def update_weights(self):\n",
    "#         self.t += 1\n",
    "#         for i, layer in enumerate(self.model.layers):\n",
    "#                 grad_w = layer.grad_w\n",
    "#                 self.m_w[i] = self.beta_1 * self.m_w[i] + (1 - self.beta_1) * grad_w\n",
    "#                 self.v_w[i] = self.beta_2 * self.v_w[i] + (1 - self.beta_2) * (grad_w ** 2)\n",
    "#                 m_hat_w = self.m_w[i] / (1 - self.beta_1 ** self.t)\n",
    "#                 v_hat_w = self.v_w[i] / (1 - self.beta_2 ** self.t)\n",
    "#                 layer.w -= self.lr * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "\n",
    "#     def update_biases(self):\n",
    "#         for i, layer in enumerate(self.model.layers):\n",
    "#                 grad_b = layer.grad_b\n",
    "#                 self.m_b[i] = self.beta_1 * self.m_b[i] + (1 - self.beta_1) * grad_b\n",
    "#                 self.v_b[i] = self.beta_2 * self.v_b[i] + (1 - self.beta_2) * (grad_b ** 2)\n",
    "#                 m_hat_b = self.m_b[i] / (1 - self.beta_1 ** self.t)\n",
    "#                 v_hat_b = self.v_b[i] / (1 - self.beta_2 ** self.t)\n",
    "#                 layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "\n",
    "#     def update(self):\n",
    "#         self.update_weights()\n",
    "#         if self.model.bias != None:\n",
    "#             self.update_biases()\n",
    "\n",
    "\n",
    "\n",
    "def parse(params, name):\n",
    "    for param_name, param_value in params.items():\n",
    "        if param_name == name:\n",
    "            return param_value\n",
    "        elif isinstance(param_value, dict):\n",
    "            return parse(param_value, name)\n",
    "        \n",
    "\n",
    "\n",
    "def AdamOptim(model, lr=lr, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-6):\n",
    "    t = 0\n",
    "    m_params = {}\n",
    "    v_params = {}\n",
    "    \n",
    "    for name, param in model.parameters().items():\n",
    "          \n",
    "        m_params[name] = np.ones_like(param)\n",
    "        v_params[name] = np.ones_like(param)\n",
    "\n",
    "\n",
    "    t += 1\n",
    "    for name, param in model.parameters().items():\n",
    "        grad = model.get_grad(name)\n",
    "        \n",
    "        m_params[name] = beta_1 * m_params[name] + (1 - beta_1) * grad\n",
    "        v_params[name] = beta_2 * v_params[name] + (1 - beta_2) * (grad ** 2)\n",
    "       \n",
    "        m_hat = m_params[name] / (1 - (beta_1 ** t))\n",
    "        v_hat = v_params[name] / (1 - (beta_2 ** t))\n",
    "        \n",
    "        \n",
    "        param -= lr * m_hat / np.sqrt(v_hat)\n",
    "        t+=1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c3a0ad6-4e1a-42c9-ad7b-d632e5175349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    def __init__(self, input_dim, sequence_length, n_head,lr = lr):\n",
    "        if input_dim % n_head != 0:\n",
    "            raise ValueError(\"Input_dim must be divisible by n_head\")\n",
    "        self.input_dim = input_dim\n",
    "        self.n_head = n_head\n",
    "        self.head_size = input_dim // n_head\n",
    "        self.sa = MultiHeadAttention(input_dim, sequence_length, head_size= self.head_size, num_heads=n_head)\n",
    "        self.ffwd = FeedForward(input_dim)\n",
    "        self.ln1 = LayerNormalization(input_dim)\n",
    "        self.ln2 = LayerNormalization(input_dim)\n",
    "        \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {'sa': self.sa.get_params(), \n",
    "                'ffwd': self.ffwd.get_params(), \n",
    "                'ln1': self.ln1.get_params(),\n",
    "                'ln2': self.ln2.get_params(),\n",
    "               }\n",
    "        \n",
    "    def get_grad(self):\n",
    "        return {\n",
    "                'grad_sa': self.sa.get_grad(), \n",
    "                'grad_ffwd': self.ffwd.get_grad(), \n",
    "                'grad_ln1': self.ln1.get_grad(), \n",
    "                'grad_ln2': self.ln2.get_grad()\n",
    "               }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.sa.zero_grad()\n",
    "        self.ffwd.zero_grad()\n",
    "        self.ln1.zero_grad()\n",
    "        self.ln2.zero_grad()\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def forward(self, x, apply_mask):\n",
    "        # print(\"Shape of x before sa: \", x.shape)\n",
    "        y = self.sa(x, apply_mask)\n",
    "        # print(\"shape of y after  SA: \", y.shape)\n",
    "        y = self.ln1(x+y) \n",
    "        # print(\"In block layerNorm 1 x shape: \", x.shape)\n",
    "        z = self.ffwd(y)\n",
    "        # print(\"In block ffw y shape: \", y.shape)\n",
    "        out = self.ln2(z+y) #apply residual connection\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dL_dy): #dL_dy represens the gradient output taken as parameter \n",
    "        \n",
    "        # Backward pass through the second Layer Normalization\n",
    "        # print(\"input of ln2: \", dL_dy.shape)\n",
    "        dL_dx = self.ln2.backward(dL_dy)\n",
    "        dL_dln2 = self.ln2.get_grad()\n",
    "        # print(\"After backprop through ln2: \", dL_dx.shape, len(dL_dln2))\n",
    "        \n",
    "        # Backward pass through the Feed Forward network\n",
    "        # print(\"input of ffwd: \", dL_dx.shape)\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.ffwd.backward(dL_dy, self.ffwd.output_activation)\n",
    "        dL_dffwd = self.ffwd.get_grad()\n",
    "        # print(\"After backprop through ffwd: \", dL_dx.shape, len(dL_dffwd))\n",
    "\n",
    "        # Backward pass through the first Layer Normalization\n",
    "        # print(\"input of ln1: \", dL_dx.shape)\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.ln1.backward(dL_dy)\n",
    "        dL_dln1 = self.ln1.get_grad()\n",
    "        # print(\"After backprop through ln1: \", dL_dx.shape, len(dL_dln1))\n",
    "\n",
    "        # Backward pass through the Self Attention mechanism\n",
    "        # print(\"input of sa backward: \", dL_dx.shape)\n",
    "        dL_dy = dL_dx\n",
    "        dL_dx = self.sa.backward(dL_dy)\n",
    "        dL_dsa = self.sa.get_grad()\n",
    "        # print(\"After backprop through SA: \", dL_dx.shape)\n",
    "\n",
    "        # # Accumulate the gradients\n",
    "        dL_dparams = {}\n",
    "        dL_dparams.update(dL_dln1)\n",
    "        dL_dparams.update(dL_dln2)\n",
    "        dL_dparams.update(dL_dffwd)\n",
    "        dL_dparams.update(dL_dsa)\n",
    "        \n",
    "        return dL_dx, dL_dparams\n",
    "\n",
    "    def update(self):\n",
    "        self.sa.update()\n",
    "        self.ffwd.update()\n",
    "        self.ln1.update()\n",
    "        self.ln2.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "988766d3-c0fa-4aa0-addc-b2dedb0d861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention:\n",
    "    # Multiple Heads of self-Attention in parallel \n",
    "    \n",
    "    def __init__(self, input_dim, sequence_length, head_size, num_heads, lr=lr):\n",
    "        self.heads = ([Head(input_dim = input_dim, sequence_length=sequence_length, head_size=head_size, lr=lr) for _ in range(num_heads)])\n",
    "        self.n_heads = num_heads\n",
    "        self.proj = Linear(head_size * num_heads, input_dim, lr=lr)\n",
    "        \n",
    "        # Actually head_size*num_heads is equal to n_embd but by proceeding like this we add another learnable param the bias\n",
    "    \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "        \n",
    "    def get_grad(self):\n",
    "        grads = {'grad_proj': self.proj.get_grad()}\n",
    "        for i, head in enumerate(self.heads):\n",
    "            grads[f'grad_head_{i}'] = head.get_grad()\n",
    "        return grads\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        for head in self.heads:\n",
    "            head.zero_grad()\n",
    "        self.proj.zero_grad()\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'proj': self.proj.get_params()\n",
    "                 }\n",
    "        for i, head in enumerate(self.heads):\n",
    "            params[f'head_{i}'] = head.get_params()\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def forward(self, x, apply_mask):\n",
    "        out = np.concatenate([h(x, apply_mask) for h in self.heads], axis=-1) # concatenate along the (batch_size, sqlength, F): F been the feature dimension \n",
    "        out = self.proj(out)  # Pass the concatenated output through the projection layer\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        # print(\"grad_out parameter: \", grad_out.shape)\n",
    "        \n",
    "        # Backpropagate through the linear projection\n",
    "        # print(\"shape of grad_proj w: \",self.proj.w.shape)\n",
    "        # print(\"shape of grad_proj b: \",self.proj.b.shape)\n",
    "        # print(\"shape of grad_proj grad_w: \",self.proj.grad_w.shape)\n",
    "        # print(\"shape of grad_proj grad_b: \",self.proj.grad_b.shape)\n",
    "        \n",
    "\n",
    "        grad_proj = self.proj.backward(grad_out)\n",
    "        # print(\"back propagate projections output: \", grad_proj.shape)\n",
    "\n",
    "        # Backpropagate through individual attention heads\n",
    "        grad_proj_split = np.split(grad_proj, self.n_heads, axis=-1)\n",
    "        # print(\"Shape after proj split: \", len(grad_proj_split), len(grad_proj_split[0]), )\n",
    "        heads = [head.backward(gp) for head, gp in zip(self.heads, grad_proj_split)]\n",
    "        for i, h in enumerate(heads):\n",
    "            # print(f\"shape of head-{i} output: \",h.shape)\n",
    "            out = np.concatenate([h for h in heads], axis=-1)\n",
    "        return out\n",
    "        \n",
    "    def update(self):\n",
    "        for head in self.heads:\n",
    "            head.update()\n",
    "        # print(\"__________________________________________________\")\n",
    "        # print(\"shape of grad_proj w: \",self.proj.w.shape)\n",
    "        # print(\"shape of grad_proj b: \",self.proj.b.shape)\n",
    "        # print(\"shape of grad_proj grad_w: \",self.proj.grad_w.shape)\n",
    "        # print(\"shape of grad_proj grad_b: \",self.proj.grad_b.shape)\n",
    "        self.proj.update()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b59bdc-f282-4dc5-bb63-8456beaf70ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9ddd41c-2b28-4975-90eb-392aaa377749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head:\n",
    "    def __init__(self, head_size, input_dim,  sequence_length, mask=None, lr=lr, bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = sequence_length\n",
    "        self.head_size = head_size\n",
    "        self.Q = Linear(input_dim, head_size, bias, lr=lr) \n",
    "        self.K = Linear(input_dim, head_size, bias, lr=lr)\n",
    "        self.V = Linear(input_dim, input_dim, bias, lr=lr)\n",
    "        self.linear_layer = Linear(input_dim, head_size, bias, lr=lr)\n",
    "        self.lr =  lr\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.mask = self.set_mask(mask)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        c = - np.max(x)\n",
    "        e_x = np.exp(x + c)\n",
    "        return e_x / np.sum(e_x)\n",
    "    \n",
    "    def __call__(self, x, apply_mask):\n",
    "        return self.forward(x, apply_mask)\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'Q': self.Q.get_params(), \n",
    "                  'K': self.K.get_params(),\n",
    "                  'V': self.V.get_params(),\n",
    "                  'linear_layer': self.linear_layer.get_params()\n",
    "                 }\n",
    "        return params\n",
    "    \n",
    "    def get_grad(self):\n",
    "        return {'grad_Q': self.Q.get_grad(), \n",
    "                'grad_K': self.K.get_grad(), \n",
    "                'grad_V': self.V.get_grad(), \n",
    "                'grad_linear_layer': self.linear_layer.get_grad()\n",
    "               }\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.Q.zero_grad()\n",
    "        self.K.zero_grad()\n",
    "        self.V.zero_grad()\n",
    "        self.linear_layer.zero_grad()\n",
    "        \n",
    "    def set_mask(self, mask=None):\n",
    "        if mask is not None:\n",
    "            self.mask = mask\n",
    "            return mask\n",
    "        mask = np.tril(np.ones((self.seq_length, self.seq_length)))\n",
    "        mask[mask == 0] = -math.inf \n",
    "        mask[mask == 1] = 0\n",
    "        self.mask = mask\n",
    "        return mask\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        dk = Q.shape[-1]\n",
    "        scaled = np.einsum('bij,bkj->bik', Q, K) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "           scaled = scaled + mask\n",
    "        attention = self.softmax(scaled)\n",
    "        out = np.einsum('bik,bkj->bij', attention, V)\n",
    "        return out, attention\n",
    "    \n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention_backward(self, grad_values):\n",
    "        dk = self.q.shape[-1]\n",
    "        \n",
    "        v_r = self.v.transpose((0,2,1))\n",
    "        k_r = self.k\n",
    "        q_r = self.q\n",
    "        \n",
    "        grad_attention = np.matmul(grad_values, v_r)\n",
    "        # print(\"grad_attention, self attention: \", grad_attention.shape, self.attention.shape)\n",
    "        \n",
    "      \n",
    "        grad_Q = np.matmul(grad_attention, k_r) / math.sqrt(dk)\n",
    "        # print(\"grad_Q: \", grad_Q.shape)\n",
    "    \n",
    "        grad_K = np.matmul(grad_attention, q_r)\n",
    "        # print(\"grad_K \", grad_K.shape)\n",
    "        \n",
    "        grad_V = np.matmul(self.attention, grad_values)\n",
    "        # print(\"grad_V: \", grad_V.shape)\n",
    "    \n",
    "        return grad_Q, grad_K, grad_V\n",
    "\n",
    "\n",
    "    def forward(self, x, apply_mask): \n",
    "        # print(\"Shape of x input head: \", x.shape)\n",
    "        self.q = self.Q(x)\n",
    "        self.k = self.K(x)\n",
    "        self.v = self.V(x)\n",
    "\n",
    "        if apply_mask:\n",
    "            values, attention = self.scaled_dot_product_attention(self.q, self.k, self.v, self.mask)\n",
    "        else:\n",
    "            values, attention = self.scaled_dot_product_attention(self.q, self.k, self.v, None)\n",
    "            \n",
    "        # print(\"shape ox x input in sda: \",x.shape)\n",
    "        # print(\"shape of sda values: \", values.shape)\n",
    "        # print(\"shape of sda attention: \", attention.shape)\n",
    "        \n",
    "        out = self.linear_layer(values)\n",
    "        self.attention = attention\n",
    "        self.values = values\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # print(\"self.out.shape: \", self.out.shape)\n",
    "        \n",
    "        grad_output = grad_output.reshape(self.out.shape)\n",
    "        grad_values = self.linear_layer.backward(grad_output)\n",
    "        \n",
    "        # print(\"grad_values shape: \",grad_values.shape)\n",
    "        \n",
    "        if grad_values is not None:\n",
    "            d_values = grad_values.copy()\n",
    "        else:\n",
    "            raise(\"grad_values is None\")\n",
    "            \n",
    "\n",
    "        grad_Q, grad_K, grad_V = self.scaled_dot_product_attention_backward(d_values)\n",
    "        # print(\"grad_Q, grad_K, grad_V: \",grad_Q.shape, grad_K.shape, grad_V.shape, \"self.q, self.k, self.v: \",self.q.shape, self.k.shape, self.v.shape)\n",
    "\n",
    "        \n",
    "        # Yould modify to compute all gradients at once\n",
    "        d_q = self.Q.backward(grad_Q)\n",
    "        d_k = self.K.backward(grad_K) \n",
    "        d_v = self.V.backward(grad_V)\n",
    "\n",
    "        # print(\"shapes d_q, d_k, d_v: \", d_q.shape, d_k.shape, d_v.shape)\n",
    "        # print(\"self.Q.grad_w, self.K.grad_w, self.V.grad_w: \",\n",
    "        #       self.Q.grad_w.shape, self.K.grad_w.shape, self.V.grad_w.shape, \"self.q, self.k, self.v: \",\n",
    "        #       self.q.shape, self.k.shape, self.v.shape)\n",
    "\n",
    "        # print(\"self.Q.grad_b, self.K.grad_b, self.V.grad_b: \",\n",
    "        #   self.Q.grad_b.shape, self.K.grad_b.shape, self.V.grad_b.shape, \"self.q, self.k, self.v: \",\n",
    "        #   self.q.shape, self.k.shape, self.v.shape)\n",
    "\n",
    "        if self.bias == True:\n",
    "            self.Q.grad_b = np.mean(self.Q.grad_b, axis=(0))\n",
    "            self.K.grad_b = np.mean(self.K.grad_b, axis=(0))\n",
    "            self.V.grad_b = np.mean(self.V.grad_b, axis=(0))\n",
    "            self.linear_layer.grad_b = np.mean(self.linear_layer.grad_b, axis=(0))\n",
    "\n",
    "        self.Q.grad_w = np.mean(self.Q.grad_w, axis=(1,2))\n",
    "        # self.Q.update_parameters(self.lr)  # Update the parameters of Q\n",
    "    \n",
    "        self.K.grad_w = np.mean(self.K.grad_w, axis=(1,2))\n",
    "        \n",
    "        # self.K.update_parameters(self.lr)  # Update the parameters of K\n",
    "    \n",
    "        self.V.grad_w = np.mean(self.V.grad_w, axis=(1,2))\n",
    "        # self.V.update_parameters(self.lr)  # Update the parameters of V\n",
    "    \n",
    "        self.linear_layer.grad_w = np.mean(self.linear_layer.grad_w, axis=(1,2))\n",
    "        \n",
    "        return grad_output\n",
    "\n",
    "    def update(self):\n",
    "        self.Q.update()\n",
    "        self.K.update()\n",
    "        self.V.update()\n",
    "        self.linear_layer.update()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fb02ff7-fc77-45ed-8967-2b1ef2fb0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:  #ok\n",
    "    def __init__(self, input_dim, lr=lr, bias=True, tol=1e-6):\n",
    "        assert isinstance(input_dim, int), \"Input size must be an integer\"\n",
    "        assert input_dim > 0, \"Input size must be positive\"\n",
    "        assert isinstance(lr, (int, float)), \"Learning rate must be a number\"\n",
    "        assert lr > 0, \"Learning rate must be positive\"\n",
    "\n",
    "        self.layers = [\n",
    "            Linear(input_dim, 4 * input_dim, bias, lr=lr),\n",
    "            Linear(4 * input_dim, input_dim, bias, lr=lr),\n",
    "        ]\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = 4 *input_dim\n",
    "        self.output_size = input_dim\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.tol=tol\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        learnable_layers = []\n",
    "        for layer in self.layers:\n",
    "            learnable_layers.append(layer.get_params())\n",
    "            \n",
    "        params['layers'] = learnable_layers\n",
    "        return params\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {'grad_layers': [layer.get_grad() for layer in self.layers]}\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        # print(\"Feedforward input x: \", x.shape)\n",
    "        self.hidden = np.dot(x, self.layers[0].w) + self.layers[0].b\n",
    "        self.hidden_activation = self.sigmoid(self.hidden)\n",
    "        self.output = np.dot(self.hidden_activation, self.layers[1].w) + self.layers[1].b\n",
    "        # print(\"Feedforward output one: \", self.output.shape)\n",
    "        self.output_activation = self.sigmoid(self.output)\n",
    "        # print(\"Feedforward output activation: \", self.output_activation.shape)\n",
    "        return self.output_activation\n",
    "            \n",
    " \n",
    "    def backward(self, y, y_pred):\n",
    "        if y is None or y_pred is None:\n",
    "            print(\"Returning None cause y or Y_PRED are None\")\n",
    "            return None\n",
    "            \n",
    "        dy = (y_pred - y) * self.sigmoid_derivative(self.output)\n",
    "        \n",
    "        dh = np.dot(dy, self.layers[1].w.T) * self.sigmoid_derivative(self.hidden)\n",
    "\n",
    "        # print(\"self.hidden shape: \", self.hidden.shape)\n",
    "        # print(\"dy shape: \", dy.shape)\n",
    "        # print(\"dh shape: \", dh.shape)\n",
    "        \n",
    "        # print(\" shape of w0: \", self.layers[0].w.shape)\n",
    "        # print(\" shape of w1: \", self.layers[1].w.shape)\n",
    "        \n",
    "        ha_reshape =  self.hidden.reshape(-1, self.hidden.shape[-1])\n",
    "        dy_reshape = dy.reshape(-1, dy.shape[-1])\n",
    "        \n",
    "        dw2 = np.dot(ha_reshape.T, dy_reshape)\n",
    "        \n",
    "        db2 = np.sum(dy, axis=0)\n",
    "        # print(\"dw2 shape: \", dw2.shape)\n",
    "        # print(\"db2 shape: \", db2.shape)\n",
    "        \n",
    "        # Reshape dh and x\n",
    "        # print(\"dh shape: \", dh.shape)\n",
    "        # print(\"self.x shape: \", self.x.shape)\n",
    "\n",
    "        dh_reshape = dh.reshape(-1, dh.shape[-1])\n",
    "        x_reshape = self.x.reshape(-1, self.x.shape[-1])\n",
    "        #appliquer une couche de grad sigmoid * deriver\n",
    "        # Calculate dw1\n",
    "        dw1 = np.dot(x_reshape.T, dh_reshape)\n",
    "        db1 = np.sum(dh, axis=0)\n",
    "\n",
    "        # print(\"dh shape: \", dh.shape)\n",
    "        # print(\"dh shape: \", dh.shape)\n",
    "       \n",
    "                \n",
    "        self.layers[1].grad_w = self.lr * dw2\n",
    "        self.layers[1].grad_b = self.lr * db2\n",
    "        self.layers[0].grad_w = self.lr * dw1\n",
    "        self.layers[0].grad_b = self.lr * db1\n",
    "        \n",
    "        return dy  \n",
    "\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af1710cb-a7ae-4f02-b4e9-e7f7499c6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:# ok\n",
    "    def __init__(self, epsilon=1e-5, tol=1e-9, lr=lr):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.tol = tol\n",
    "        self.gamma = None \n",
    "        self.beta = None\n",
    "        self.param_shape = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.param_shape = x.shape\n",
    "        self.gamma = np.ones(self.param_shape) \n",
    "        self.beta = np.zeros(self.param_shape)\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return {\n",
    "                'gamma': self.gamma,\n",
    "                'beta': self.beta, \n",
    "               }\n",
    "    def zero_grad(self):\n",
    "        #Reset all gradients to zero\n",
    "        self.grad_beta = np.zeros_like(self.beta)\n",
    "        self.grad_gamma = np.zeros_like(self.gamma)\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {\n",
    "               'grad_gamma': self.grad_gamma, \n",
    "                'grad_beta': self.grad_beta\n",
    "               }\n",
    "        \n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        # self.dims = tuple(-1 + i for i in range(len(self.param_shape)))\n",
    "        self.dims = -1 \n",
    "        self.mean = x.mean(axis=self.dims, keepdims=True)\n",
    "        self.var = ((x-self.mean) ** 2).mean(axis=self.dims, keepdims = True)\n",
    "        self.std = np.sqrt((self.var + self.epsilon))\n",
    "        self.std = np.maximum(self.std, self.tol)\n",
    "\n",
    "        self.y = (x - self.mean) / self.std\n",
    "        out =  self.gamma * self.y + self.beta\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        #compute the derivative of loss wrt y, x, mean, var and sigma\n",
    "        if grad_output is None:\n",
    "            return None\n",
    "            \n",
    "        # Compute gradients with respect to the normalized output y\n",
    "        # print(\"grad_ouput shape: \", grad_output.shape)\n",
    "        \n",
    "        # grad_output = grad_output.transpose((1, 0, 2))\n",
    "        # print(\"dim of x: \", self.x.shape, \"dim of x.T: \", self.x.T.shape, \"grad_ouput shape: \", grad_output.shape)\n",
    "        # print(\"self.gamma shape: \", self.gamma.shape)\n",
    "\n",
    "        gamma_reshape = self.gamma.reshape(self.gamma.shape)\n",
    "        \n",
    "        #x_reshape = self.x.reshape()\n",
    "        grad_y = grad_output * gamma_reshape\n",
    "        # Compute gradients with respect to the mean and variance\n",
    "\n",
    "        # print(\"self.std shape: \", self.std.shape)\n",
    "        # print(\"grad_y shape: \", grad_y.shape)\n",
    "\n",
    "        # grad_y_reshape = grad_y.reshape(batch_size, block_size, input_dim)\n",
    "        # print(\"grad_y_reshape shape: \", grad_y_reshape.shape)\n",
    "\n",
    "        grad_mean = -np.sum(grad_y, axis=self.dims, keepdims=True) / self.std\n",
    "        grad_var = -0.5 * np.sum(grad_y * (self.x - self.mean + self.epsilon), axis=self.dims, keepdims=True) / (self.var + self.epsilon)**1.5\n",
    "    \n",
    "        # Compute gradients with respect to the input x\n",
    "        grad_x = (grad_y - grad_mean - (self.x - self.mean) * grad_var) / self.std\n",
    "    \n",
    "        # Compute gradients with respect to the gamma and beta parameters\n",
    "        self.grad_gamma = np.sum(grad_y * self.y, axis=self.dims, keepdims=True)\n",
    "        self.grad_beta = np.sum(grad_y, axis=self.dims, keepdims=True)\n",
    "        \n",
    "\n",
    "        return grad_x\n",
    "\n",
    "    def update(self):\n",
    "        # self.gamma -= self.lr * self.grad_gamma[0]\n",
    "        # self.beta -= self.lr * self.grad_beta[0]      \n",
    "        AdamOptim(self, lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d54049-eae4-4d46-9286-44b43f26863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4914462f-c013-43b8-bbc4-da0d13c5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, lr=lr):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Initialize weights with Glorot uniform initialization\n",
    "        limit = np.sqrt(6 / (in_features + out_features))\n",
    "        \n",
    "        self.w = np.random.uniform(-limit, limit, (in_features, out_features))\n",
    "        if self.bias:\n",
    "            self.b = np.zeros(out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.x = None\n",
    "        self.grad_w = None\n",
    "        self.grad_b = None\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "                  'w': self.w,\n",
    "                  'b': self.b if self.bias else None\n",
    "                 }\n",
    "        return params\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # Reset all gradients to zero\n",
    "        self.grad_w = np.zeros_like(self.w)\n",
    "        if self.bias:\n",
    "            self.grad_b = np.zeros_like(self.b)\n",
    "            \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {}\n",
    "        if self.bias:\n",
    "             self.grads ={'grad_w': self.grad_w, 'grad_b': self.grad_b}\n",
    "        else:\n",
    "            self.grads = {'grad_w': self.grad_w}\n",
    "\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "       \n",
    "            \n",
    "    def forward(self, x):\n",
    "        # print(\"linear input shape: \", x.shape)\n",
    "        self.x = x\n",
    "        out = np.matmul(x, self.w)\n",
    "        if self.bias:\n",
    "            out += self.b\n",
    "        self.out = out\n",
    "        # print(\"linear forward output shape: \", out.shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if grad_output is None:\n",
    "            print('grad output is none')\n",
    "            return None\n",
    "\n",
    "        # print(\"x_reshape shape: \", x_reshaped.shape)\n",
    "        # print(\"grad_out shape: \", grad_output.shape)\n",
    "        grad_output = grad_output.transpose((1, 0, 2))\n",
    "\n",
    "        # print(\"dim of x: \", self.x.shape,  \"grad_output shape: \", grad_output.shape)\n",
    "        # print(\"grad weight linear: \", self.grad_w.shape)\n",
    "        # print(\"weight linear: \", self.w.shape)\n",
    "\n",
    "        self.grad_w = np.dot(self.x.T, grad_output) / self.x.shape[0]\n",
    "        if self.bias:\n",
    "            self.grad_b = np.sum(grad_output, axis=0) / self.x.shape[0]\n",
    "\n",
    "        grad_input = np.dot(grad_output, self.w.T)\n",
    "        grad_input = grad_input.transpose((1,0,2))\n",
    "        # print(\"grad_input at the end of linear backward: \", grad_input.shape)\n",
    "        return grad_input\n",
    "    \n",
    "    def update(self,):\n",
    "        if len(self.grad_w.shape) == 4:\n",
    "            self.grad_w = np.mean(self.grad_w, axis=(1,2))\n",
    "            # print(\"shape of grad_w: \", self.grad_w.shape)\n",
    "\n",
    "        # self.w -= self.lr * self.grad_w\n",
    "        if self.bias:\n",
    "            if len(self.grad_b.shape) == 2:\n",
    "                # print(\"shape of grad_b: \", self.grad_b.shape)\n",
    "                self.grad_b = np.mean(self.grad_b, axis=(0))\n",
    "                # self.b -= self.lr * self.grad_b\n",
    "                AdamOptim(self, lr=self.lr)\n",
    "        else:\n",
    "            self.w -= self.lr * self.grad_w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4f423-27dc-4eff-99f0-46e7aed4fa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ab3fa8e-1ca9-44f9-995c-265b29a16446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, input_dim, lr=lr):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.embeddings = np.random.randn(vocab_size, input_dim)\n",
    "        self.lr=lr\n",
    "        # Initialize weights with Glorot uniform initialization\n",
    "        limit = np.sqrt(6 / (vocab_size + input_dim))\n",
    "        self.dL_dembeddings = np.random.uniform(-limit, limit, (vocab_size, input_dim))\n",
    "\n",
    "       \n",
    "    def __call__(self, index):\n",
    "        return self.forward(index)\n",
    "    \n",
    "    def get_params(self):\n",
    "         self.params = {\n",
    "                'embeddings': self.embeddings,\n",
    "               }\n",
    "         return self.params\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.get_params()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.dL_dembeddings = np.zeros_like(self.embeddings)  # Reset the gradients to zero\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {'grad_embeddings': self.dL_dembeddings}\n",
    "        return self.grads[f'grad_{name}'] if name is not None else self.grads\n",
    "        \n",
    "    def forward(self, index):\n",
    "        self.out = self.embeddings[index]\n",
    "        return self.embeddings[index]\n",
    "\n",
    "    def backward(self, dL_dy, index):\n",
    "        self.dL_dembeddings[index] = dL_dy\n",
    "        \n",
    "        return self.dL_dembeddings\n",
    "        \n",
    "    def update(self):\n",
    "        AdamOptim(self, lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdde3c-1a30-49f1-ad80-16f744c3f4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9273a974-dbd2-403b-ab45-532e660c9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stand Alone Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3cc577a-9a93-40df-a299-0f9d26e5e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/meroem/Desktop/Bert/trans\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4786.82it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 5272.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def txt_files_in_dir(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\") and os.path.isfile(os.path.join(directory, filename)):\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "folder_path = \"/home/meroem/Desktop/Bert/oscar\"\n",
    "output_file_train = \"data/train_split.txt\"\n",
    "output_file_val = \"data/val_split.txt\"\n",
    "# output_file = \"output{}.txt\"\n",
    "vocab_file = \"data/vocab.txt\"\n",
    "# split_files = int(input(\"Howmany files would you like to split this into: \"))\n",
    "\n",
    "print(os.system(\"pwd\"))\n",
    "\n",
    "files = txt_files_in_dir(folder_path)\n",
    "total_files = len(files)\n",
    "\n",
    "#split_index = int(total_files * 0.9)\n",
    "\n",
    "\n",
    "\n",
    "# files_train = files[:split_index]\n",
    "# files_val = files[split_index:]\n",
    "files_train = files[:100]\n",
    "files_val = files[1:10]\n",
    "\n",
    "# max_count = total_files // split_files if split_files != 0 else total_files\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "with open(output_file_train, 'w', encoding=\"utf-8\") as outfile:\n",
    "    for count, filename in enumerate(tqdm(files_train, total=len(files_train))):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rt', encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            character = set(text)\n",
    "            vocab.update(character)\n",
    "with open(output_file_val, 'w', encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_val, total=len(files_val)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rt', encoding='utf-8') as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "            \n",
    "with open(vocab_file, 'w', encoding=\"utf-8\") as vfile:\n",
    "    for char in vocab:\n",
    "        vfile.write(char + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00b8cef6-495e-4766-975a-e9c780b890b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok then i am going for caracter level\n",
    "\n",
    "chars = \"\"\n",
    "with open('data/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "#with open(\"openwebtext/vocab.txt\" ,'r', encoding='utf-8') as f\n",
    "    text=f.read()\n",
    "    #print(len(text))\n",
    "    chars = sorted(list(set(text)))\n",
    "#print(chars)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {char:i for i,char in enumerate(chars)}\n",
    "itos = {i:char for i,char in enumerate(chars)}\n",
    "encode  = lambda s:[stoi[c] for c in s]\n",
    "decode = lambda l:\"\".join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "#memory map for using snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"data/train_split.txt\" if split == 'train' else \"data/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            #determine file size and a random position to start reading\n",
    "            \n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "            \n",
    "            #Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "            # block = mm.read(n*block_size*batch_size-1),  where we determine the text amount of text read \n",
    "            \n",
    "            #decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            #Train and test splits\n",
    "            data = np.array(encode(decoded_block)) \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = np.random.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x =  np.stack([data[i:i+block_size] for i in ix]) \n",
    "    y =  np.stack([data[i+1:i+block_size+1] for i in ix])# Appartir du next char\n",
    "    return x, y\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32c3311f-b6e9-41a7-b7c1-e8c11069314b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6b21833-c303-4d99-80c0-3da3257a1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Here's a full list of key shortcuts for laptops (please note that most of them are for laptop as well as for usual, i.e. \n",
    "desktop, keyboards): That was very useful for me too, I have been meaning to look for something like this for a while,\n",
    "but never actually done it. Thanks for sharing this valuable information! Create your account in minutes, and start working! \n",
    "3-month trial for agencies, and free for freelancers! The system lets you keep client/vendor database, with contacts and rates,\n",
    "manage projects and assign jobs to vendors, issue invoices, track payments, store and manage project files, generate business \n",
    "reports on turnover profit per client/manager etc.The climate in Ireland is typically temperate maritime; modified by North \n",
    "Atlantic Current; mild winters, cool summers; consistently humid; overcast about half the time. The terrain: mostly level to \n",
    "rolling interior plain surrounded by rugged hills and low mountains; sea cliffs on west coast. For Dublin in January the daily \n",
    "average maximum temperatures is 8°C with the average minimum 3°C, while in June the average maximum is 18°C with a minimum of 9°C. \n",
    "The wettest month for Dublin is October with an average of 89.0mm of precipitation falling while the driest month is March with 54.9mm \n",
    "falling. The information presented below gives detailed historical monthly average weather conditions along with exceptional weather\n",
    "occurrences. To maintain relevance to current weather trends the displayed information has been calculated using data collected over\n",
    "the past two decades. The climate profile is taken from closest available data source to Glendineoregan. Throughout the month of \n",
    "November daytime temperatures will generally reach highs of around 10°C that's about 51°F. At night the average minimum temperature\n",
    "drops down to around 5°C, that's 40°F. In recent times the highest recorded temperature in November has been 17°C that's 63°F, with the \n",
    "lowest recorded temperature -7°C, about 20°F. The Heat Index is a measure of how hot it feels when relative humidity is added to actual\n",
    "air temperature. From this a comfort level is calculated providing categories on how heat conditions might adversely affect someone.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1acadd7-8ab3-4018-b760-58a66e357d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The climate profile is taken from closest available'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(text)\n",
    "\n",
    "l, ll = tok.encode_with_lengths(\"The climate profile is taken from closest available\")\n",
    "tok.decode(l, ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e02a8ee-9fa4-4df6-86a8-d893dd0b6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "    def __init__(self, vocab_size, sequence_length=block_size, lr=3e-4):\n",
    "        self.embedding_table = Embedding(vocab_size, input_dim, lr=lr )\n",
    "        self.position_embedding_table = PositionalEncoding(max_sequence_length, input_dim)\n",
    "        self.decoder_block = [Block(input_dim,sequence_length=sequence_length, n_head=n_head, lr=lr) for _ in range(n_layer)] # Decoder Block\n",
    "        self.ln_f = LayerNormalization(input_dim,lr=lr) #Final linearNormailization\n",
    "        self.lm_head = Linear(input_dim, vocab_size,lr=lr)  #language modeling head\n",
    "    \n",
    "    def one_hot_encode(self, labels, num_classes):\n",
    "        one_hot = np.zeros((len(labels), num_classes))\n",
    "        one_hot[np.arange(len(labels)), labels] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def softmax(self, logits, axis=None, keepdims=True):\n",
    "        c = -np.max(logits)\n",
    "        denominator = np.sum(np.exp(logits+c), axis=axis, keepdims=keepdims)\n",
    "        probs = np.exp(logits + c)/denominator\n",
    "        return probs\n",
    "    \n",
    "    def derivative_softmax(self, t):\n",
    "        # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "        s = t.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "        \n",
    "    def cross_entropy(self, logits, targets, tol=1e-6):\n",
    "        N = logits.shape[0]\n",
    "        probabilities = self.softmax(logits)\n",
    "        # print(\"------------------------------------------------------\")\n",
    "        # print(probabilities)\n",
    "        ce = -np.sum(targets * np.log(probabilities + tol)) / N\n",
    "        return ce\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.embedding_table.zero_grad()\n",
    "        for block in self.decoder_block:\n",
    "             block.zero_grad()\n",
    "        self.ln_f.zero_grad()\n",
    "        self.lm_head.zero_grad()\n",
    "        \n",
    "    \n",
    "    def parameters(self):\n",
    "        self.params = {\n",
    "        'embeddings': self.embedding_table.get_params(),\n",
    "        'ln_f': self.ln_f.get_params(),\n",
    "        'lm_head': self.lm_head.get_params()\n",
    "        }\n",
    "        \n",
    "        for i, block in enumerate(self.decoder_block):\n",
    "            self.params[f'decoder_block_{i}']=block.get_params() \n",
    "        return self.params\n",
    "    \n",
    "    def __call__(self,index, targets=None, apply_mask=True):\n",
    "        return self.forward(index, targets, apply_mask)\n",
    "        \n",
    "    def forward(self, index, targets=None, apply_mask=True):\n",
    "        self.index = index\n",
    "        batch_size, time_space = index.shape\n",
    "        # print(\"batch_size :\", batch_size)\n",
    "        # print(\"time_space: \", time_space)\n",
    "        tok_embed = self.embedding_table(index) # (batch_size, time_space, input_dim)\n",
    "        # print(\"tok_embed shape\", tok_embed.shape)\n",
    "        pos_encode = self.position_embedding_table(np.arange(time_space)) # (time_space, input_dim)\n",
    "        # print(\"pos_embed shape\", pos_embed.shape)\n",
    "        pos_encode = np.expand_dims(pos_encode, axis=0)  # (1, time_space, input_dim)\n",
    "        # print(\"pos_embed shape after expand\", pos_embed.shape)\n",
    "        x = tok_embed + pos_encode  # (batch_size, time_space, input_dim)\n",
    "        # print(\"x shape: \", x.shape)\n",
    "        # print(\"---------------------------------------------------------------------------------------------\")\n",
    "        for b in self.decoder_block:\n",
    "            x = b(x, apply_mask)\n",
    "        # print(\"---------------------------------------------------------------------------------------------\")\n",
    "        # print(\"x shape after decoder: \", x.shape)\n",
    "        # print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        # print(\"x shape after final layer norm: \", x.shape)\n",
    "        \n",
    "        logits = self.lm_head(x) # (batch_size, time_space, vocab_size)\n",
    "        # print(\"logits shape after linear laguage modeling: \", logits.shape)\n",
    "      \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch_size, time_space, vocab_size = logits.shape\n",
    "\n",
    "            #Blend the logits and vocab channels together\n",
    "\n",
    "            logits = logits.reshape(batch_size * time_space, vocab_size)\n",
    "            targets = targets.reshape(batch_size * time_space)\n",
    "            \n",
    "            '''Logits refers to unnormalized output scores'''\n",
    "            # print(\"logits shape   reshaping head: \",logits.shape)\n",
    "            # print(\"targets shape   reshaping head: \",targets.shape)\n",
    "\n",
    "            '''Implement cross entropy'''\n",
    "            one_hot_targets = self.one_hot_encode(targets, num_classes=vocab_size)\n",
    "            # print(\"one hot encode targets: \", one_hot_targets.shape)\n",
    "            # print(\"logits in forward\",logits[:10])\n",
    "            loss = model.cross_entropy(logits, one_hot_targets)\n",
    "            # print(\"cross entropy in forward: \", loss)\n",
    "\n",
    "            # print(\"loss: \", loss.shape)\n",
    "            if loss is None:\n",
    "                print(\"loss is none\")\n",
    "            if logits is None:\n",
    "                print(\"logits is none\")\n",
    "                print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        self.output = logits # store output for backward pass      \n",
    "        return logits, loss\n",
    "    \n",
    "    #implement the backward pass for the gpt model following the structure we have defined\n",
    "    def backward(self):\n",
    "\n",
    "        #Add softmax derivative by Dy\n",
    "        dL_dy = np.ones_like(self.lm_head.out) # derivative of loss wrt y where y is the model output\n",
    "        # print(\"dL-dy: \", dL_dy.shape)\n",
    "        \n",
    "        # Backward pass through the language modeling head\n",
    "        dL_dx = self.lm_head.backward(dL_dy) #obtain derivative of Loss wrt input x and wrt lm modeling head output\n",
    "        dL_dlm_head = self.lm_head.get_grad()\n",
    "\n",
    "        # Backward pass through the final Layer Normalization\n",
    "        dL_dy = dL_dx #set up the derivative of loss wrt model input at lm head\n",
    "        dL_dx = self.ln_f.backward(dL_dy) #get derivative of L wrt to the linear layer input\n",
    "        dL_dln_f = self.ln_f.get_grad()\n",
    "        \n",
    "        # Backward pass through the Decoder Blocks\n",
    "        dL_dy = dL_dx #set derivative L wrt linear layer as input for decoder\n",
    "        dL_ddecoder_block = [] #list of various decoder losses\n",
    "        \n",
    "        for block in reversed(self.decoder_block):\n",
    "            dL_dx, dL_decoder = block.backward(dL_dy)  #get derivative of each block and set is as input to previous block\n",
    "            dL_dblock = block.get_grad()\n",
    "            dL_dy = dL_dx\n",
    "            dL_ddecoder_block.append(dL_dblock) # then append to the list\n",
    "       \n",
    "        # Backward pass through the embedding table\n",
    "        dL_dy = dL_dx #set pos-embd output as input to embedding layer\n",
    "        dL_dx = self.embedding_table.backward(dL_dy, self.index)#get derivative of loss wrt to embeding table output\n",
    "        dL_dembedding_table = self.embedding_table.get_grad()\n",
    "       \n",
    "        # Accumulate the gradients for update by the custom adam otimizer\n",
    "        # dL_dparams = {}\n",
    "        # dL_dparams.update(dL_dembedding_table)\n",
    "        # dL_dparams.update(dL_ddecoder_block)\n",
    "        # dL_dparams.update(dL_decoder)\n",
    "        # dL_dparams.update(dL_dln_f)\n",
    "        # dL_dparams.update(dL_dlm_head)\n",
    "        # d_dparams.update(dL_dparams)\n",
    "                \n",
    "        #return dL_dx#, dL_dparams\n",
    "        \n",
    "    def generate(self, index, max_new_tokens = input_dim):\n",
    "        mode = 'val'\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:] #crop index to the last block_size tokens  # (batch_size, block_size)\n",
    "\n",
    "            #get the predictions\n",
    "            logits, loss = self.forward(index_cond, apply_mask=False)\n",
    "            \n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :]#becomes # (batch_size, vocab_size)\n",
    "            \n",
    "            #apply softmax to get probabilities\n",
    "            probs = self.softmax(logits)#  # (batch_size, vocab_size)\n",
    "            \n",
    "            #sample from the distribution\n",
    "            index_next = np.array([np.random.choice(range(vocab_size), p=probs[i]) for i in range(probs.shape[0])]).reshape(-1, 1)\n",
    "            # index = np.concatenate((index, index_next), axis=1)# (batch_size, time_space + 1)\n",
    "\n",
    "            #greedy decoding\n",
    "            #index_next = np.argmax(logits, axis=-1).reshape(-1, 1)\n",
    "            index = np.concatenate((index, index_next), axis=1)# (batch_size, time_space + 1)\n",
    "        return index, loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        mode='train'\n",
    "        losses = {'train': [],}\n",
    "        for epoch in range(epochs):\n",
    "            for split in ['train', ]:\n",
    "                inputs, targets = get_batch(split)\n",
    "                # print(\"Entring the forward pass\")\n",
    "                logits, loss = self.forward(inputs, targets)\n",
    "                # print(\"Logits 10: \",logits[:10])\n",
    "                # if split == 'train':\n",
    "                # print(\"Entring the backward pass\")\n",
    "                self.zero_grad()\n",
    "\n",
    "                self.backward()\n",
    "                # print(\"Finished backward\")\n",
    "                \n",
    "                self.update() \n",
    "                # print(\"Updated parameters\")\n",
    "\n",
    "                losses[split].append(loss.item())\n",
    "            if epoch % evals == 0:                \n",
    "                print(f\"Epoch :{epoch}/{epochs} train_loss: {np.mean(losses['train']):.8f}\")\n",
    "        print(f\"\\n\\n Final train_loss: {np.mean(losses['train']):.8f}\")\n",
    "        \n",
    "\n",
    "    def update(self):\n",
    "        self.embedding_table.update()\n",
    "        for block in self.decoder_block:\n",
    "             block.update()\n",
    "        self.ln_f.update()\n",
    "        self.lm_head.update()\n",
    "        \n",
    "    def get_grad(self, name=None):\n",
    "        self.grads = {\n",
    "                      'grad_embeddings':self.embedding_table.get_grad(), \n",
    "                      'grad_lm_head': self.lm_head.get_grad(),\n",
    "                      'grad_ln_f': self.ln_f.get_grad()\n",
    "        }\n",
    "        for i, block in enumerate(self.decoder_block):\n",
    "           self.grads[f'grad_decoder_block_{i}'] = block.get_grad()\n",
    "\n",
    "        if name == None:\n",
    "            return self.grads\n",
    "        return parse(self.grads, f'grad_{name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ff58df2-e2aa-4a7a-acd7-45a2a09e9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc27c185-2281-49cb-88a2-88a1d6b79bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded!\n"
     ]
    }
   ],
   "source": [
    "@staticmethod\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model Loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model Saved!\")\n",
    "\n",
    "\n",
    "\n",
    "model = load('model.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7179bfb-3e9a-484d-aa53-518cb262b8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bde98080-9e8f-4c13-a275-8e0825f7eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35967/3742030871.py:104: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0/10000 train_loss: 8.72225852\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs)  \n",
    "save(model, 'model.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d98f5a-6677-4d4c-94ab-1976b0b393c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe70dd3-b10b-4d6c-b2ba-9b4426e81894",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59562ce7-2887-4535-8211-403001756189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35967/3742030871.py:104: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "context = np.array(encode(prompt))[np.newaxis, :]\n",
    "generated_chars, loss = model.generate(context, max_new_tokens=20)\n",
    "generated_chars = generated_chars[0].tolist()\n",
    "generated_text = decode(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d44a4d-1837-4bd7-8a77-40b6163b47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Man\n",
      "Generated text: Manyผā→ï🤗£3вd+Ü~‍oк_SpG\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\", prompt)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c866b-2699-4822-bad5-3b65a6f647bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761381f1-e809-438a-80ec-c7fa92a7e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901714d-0f0e-4fd7-b8d4-dd79bb7e44a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efcd05-a91b-41a6-8b24-f7f84e095ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff029e-7154-423d-864f-15aa93718453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
